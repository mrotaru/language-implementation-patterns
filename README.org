* Langauge Implementation Patterns

** LL(1) Recursive Descent Lexer
   - the terms "lexer", "tokenizer" and "scanner" are used interchangeably
   - implement a tokenizer for list representations like "[a, b]", "[]", "[a,b,c]", etc
     
*** Token Types
    - a ~list_tokenizer()~ function borrows the text to tokenize, returing an array of ~Token~ objects
    - token types: ~Comma~, ~ListItem~, ~SquareBracketOpen~, ~SquareBracketClose~, ~EOF~
    #+NAME: token_types
    #+BEGIN_SRC rust
      #[derive(Debug, Clone)]
      enum Token {
          Comma,
          SquareBracketOpen,
          SquareBracketClose,
          ListItem { payload: RefCell<String> },
          EOF,
      }
    #+END_SRC
    
*** List Item Tokens
    - most tokens are very simple, one character and can simply be returned when encountered
    - list item tokens have the actual text as the payload
    - letters (Unicode code points with the "ALPHABETIC" property) are the only valid list item characters
    - unlike with single-char tokens, we can't simply advance the iterator when it comes to multi-char ones, as this might result in a non-item character being consumed and not being available on the next loop
    - the following snippet handles the case when the current character is not one of the known single-character tokens:
    #+NAME: list_item_token
    #+BEGIN_SRC rust
      if current_char.is_alphabetic() {
          let mut item = String::new().to_owned();
          item.push_str(&current_char.to_string());
          while let Some(inner_char) = iterator.peek() {
              if inner_char.is_alphabetic() {
                  item.push_str(&inner_char.to_string());
                  iterator.next();
              } else {
                  break
              }
          }
          return Ok(RefCell::new(Token::ListItem { payload: RefCell::new(item) }));
      } else {
          return Err(format!("Invalid character: '{}'", current_char));
      }
    #+END_SRC
    
*** Main Loop
    - repeatedly invokes the ~next_token()~ closure, which returns a token (or an error)
    - the returned token will be wrapped in a ~RefCell~ - so we use ~into_inner()~ to get the wrapped ~Token~
    - a clone of the token is then pushed into the array of tokens to be returned
    - the ~EOF~ token type is used to signal that the input text has been tokenized - when it is encountered, we return the array of tokens gathered so far
    - if the token is not ~EOF~, we simply start another iteration of the loop
    #+NAME: main_loop
    #+BEGIN_SRC rust
      loop {
          match next_token() {
              Ok(token_ref) => {
                  let token = token_ref.into_inner();
                  tokens.push(RefCell::new(token.clone()));
                  match token {
                      Token::EOF => { return Ok(tokens) },
                      _ => {
                          continue;
                      }
                  };
              },
              Err(error) => { return Err(error) },
          };
      };
    #+END_SRC
    
*** Tests

    #+NAME: tests
    #+BEGIN_SRC rust
      let tokens = list_tokenizer("").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      assert!(matches!(tokens[0], Token::EOF));

      let tokens = list_tokenizer(";");
      assert!(matches!(tokens, Err(_)));

      let tokens = list_tokenizer("[]").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      assert!(matches!(tokens[0], Token::SquareBracketOpen));
      assert!(matches!(tokens[1], Token::SquareBracketClose));
      assert!(matches!(tokens[2], Token::EOF));

      let tokens = list_tokenizer("foo").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      //assert!(matches!(tokens[0], Token::ListItem { payload: RefCell::new(String::from("foo")) })); // fn calls are not allowed in patterns
      let foo = RefCell::new(String::from("foo"));
      assert!(matches!(&tokens[0], Token::ListItem { payload: foo }));
      assert!(matches!(tokens[1], Token::EOF));

      let tokens = list_tokenizer("[foo]").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      let foo = RefCell::new(String::from("foo"));
      assert!(matches!(tokens[0], Token::SquareBracketOpen));
      assert!(matches!(&tokens[1], Token::ListItem { payload: foo }));
      assert!(matches!(tokens[2], Token::SquareBracketClose));
      assert!(matches!(tokens[3], Token::EOF));

      let tokens = list_tokenizer("[foo, bar]").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      let foo = RefCell::new(String::from("foo"));
      let bar = RefCell::new(String::from("bar"));
      assert!(matches!(tokens[0], Token::SquareBracketOpen));
      assert!(matches!(&tokens[1], Token::ListItem { payload: foo }));
      assert!(matches!(tokens[2], Token::Comma));
      assert!(matches!(&tokens[3], Token::ListItem { payload: bar }));
      assert!(matches!(tokens[4], Token::SquareBracketClose));
      assert!(matches!(tokens[5], Token::EOF));
    #+END_SRC

    #+RESULTS: tests

*** Complete Tokenizer
    #+BEGIN_SRC rust :noweb yes :tangle 01-LL1-recursive-descent-parser.rs
      use std::cell::RefCell;

      <<token_types>>

      fn list_tokenizer (input: &str) -> Result<Vec<RefCell<Token>>, String> {
          let mut iterator = input.chars().peekable();
          let mut tokens = vec![];

          let mut next_token = || -> Result<RefCell<Token>, String> {
              loop {
                  let current_char_option = iterator.next();
                  match current_char_option {
                      Some(current_char) => {
                          if current_char.is_whitespace() {
                              continue;
                          } else {
                              match current_char {
                                  ',' => { return Ok(RefCell::new(Token::Comma)) },
                                  '[' => { return Ok(RefCell::new(Token::SquareBracketOpen)) },
                                  ']' => { return Ok(RefCell::new(Token::SquareBracketClose)) },
                                  _ => {
                                      <<list_item_token>>
                                  }
                              }
                          }
                      },
                      None => {
                          return Ok(RefCell::new(Token::EOF));
                      },
                  }
              }
          };

          <<main_loop>>
      }

      <<tests>>
    #+END_SRC

    #+RESULTS:


