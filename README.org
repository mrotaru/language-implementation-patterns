* Langauge Implementation Patterns
** LL(1) Recursive Descent Lexer
   - the terms "lexer", "tokenizer" and "scanner" are used interchangeably
   - implement a tokenizer for list representations like "[a, b]", "[]", "[a,b,c]", etc
     
*** Token Types
    - a ~list_tokenizer()~ function borrows the text to tokenize, returing an array of ~Token~ objects
    - token types: ~Comma~, ~ListItem~, ~SquareBracketOpen~, ~SquareBracketClose~, ~EOF~
    #+NAME: token_types
    #+BEGIN_SRC rust
      #[derive(Debug, Clone)]
      enum Token {
          Comma,
          SquareBracketOpen,
          SquareBracketClose,
          ListItem { payload: RefCell<String> },
          EOF,
      }
    #+END_SRC
    
*** List Item Tokens
    - most tokens are very simple, one character and can simply be returned when encountered
    - list item tokens have the actual text as the payload
    - letters (Unicode code points with the "ALPHABETIC" property) are the only valid list item characters
    - unlike with single-char tokens, we can't simply advance the iterator when it comes to multi-char ones, as this might result in a non-item character being consumed and not being available on the next loop
    - the following snippet handles the case when the current character is not one of the known single-character tokens:
    #+NAME: list_item_token
    #+BEGIN_SRC rust
      if current_char.is_alphabetic() {
          let mut item = String::new().to_owned();
          item.push_str(&current_char.to_string());
          while let Some(inner_char) = iterator.peek() {
              if inner_char.is_alphabetic() {
                  item.push_str(&inner_char.to_string());
                  iterator.next();
              } else {
                  break
              }
          }
          return Ok(RefCell::new(Token::ListItem { payload: RefCell::new(item) }));
      } else {
          return Err(format!("Invalid character: '{}'", current_char));
      }
    #+END_SRC
    
*** Main Loop
    - repeatedly invokes the ~next_token()~ closure, which returns a token (or an error)
    - the returned token will be wrapped in a ~RefCell~ - so we use ~into_inner()~ to get the wrapped ~Token~
    - a clone of the token is then pushed into the array of tokens to be returned
    - the ~EOF~ token type is used to signal that the input text has been tokenized - when it is encountered, we return the array of tokens gathered so far
    - if the token is not ~EOF~, we simply start another iteration of the loop
    #+NAME: main_loop
    #+BEGIN_SRC rust
      loop {
          match next_token() {
              Ok(token_ref) => {
                  let token = token_ref.into_inner();
                  tokens.push(RefCell::new(token.clone()));
                  match token {
                      Token::EOF => { return Ok(tokens) },
                      _ => {
                          continue;
                      }
                  };
              },
              Err(error) => { return Err(error) },
          };
      };
    #+END_SRC
    
*** Tests

    #+NAME: tests
    #+BEGIN_SRC rust
      let tokens = list_tokenizer("").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      assert!(matches!(tokens[0], Token::EOF));

      let tokens = list_tokenizer(";");
      assert!(matches!(tokens, Err(_)));

      let tokens = list_tokenizer("[]").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      assert!(matches!(tokens[0], Token::SquareBracketOpen));
      assert!(matches!(tokens[1], Token::SquareBracketClose));
      assert!(matches!(tokens[2], Token::EOF));

      let tokens = list_tokenizer("foo").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      //assert!(matches!(tokens[0], Token::ListItem { payload: RefCell::new(String::from("foo")) })); // fn calls are not allowed in patterns
      let foo = RefCell::new(String::from("foo"));
      assert!(matches!(&tokens[0], Token::ListItem { payload: foo }));
      assert!(matches!(tokens[1], Token::EOF));

      let tokens = list_tokenizer("[foo]").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      let foo = RefCell::new(String::from("foo"));
      assert!(matches!(tokens[0], Token::SquareBracketOpen));
      assert!(matches!(&tokens[1], Token::ListItem { payload: foo }));
      assert!(matches!(tokens[2], Token::SquareBracketClose));
      assert!(matches!(tokens[3], Token::EOF));

      let tokens = list_tokenizer("[foo, bar]").unwrap().into_iter().map(|r| r.into_inner()).collect::<Vec<_>>();
      let foo = RefCell::new(String::from("foo"));
      let bar = RefCell::new(String::from("bar"));
      assert!(matches!(tokens[0], Token::SquareBracketOpen));
      assert!(matches!(&tokens[1], Token::ListItem { payload: foo }));
      assert!(matches!(tokens[2], Token::Comma));
      assert!(matches!(&tokens[3], Token::ListItem { payload: bar }));
      assert!(matches!(tokens[4], Token::SquareBracketClose));
      assert!(matches!(tokens[5], Token::EOF));
    #+END_SRC

*** Complete Tokenizer
    #+BEGIN_SRC rust :noweb yes :tangle 01-LL1-recursive-descent-parser.rs
      use std::cell::RefCell;

      <<token_types>>

      fn list_tokenizer (input: &str) -> Result<Vec<RefCell<Token>>, String> {
          let mut iterator = input.chars().peekable();
          let mut tokens = vec![];

          let mut next_token = || -> Result<RefCell<Token>, String> {
              loop {
                  let current_char_option = iterator.next();
                  match current_char_option {
                      Some(current_char) => {
                          if current_char.is_whitespace() {
                              continue;
                          } else {
                              match current_char {
                                  ',' => { return Ok(RefCell::new(Token::Comma)) },
                                  '[' => { return Ok(RefCell::new(Token::SquareBracketOpen)) },
                                  ']' => { return Ok(RefCell::new(Token::SquareBracketClose)) },
                                  _ => {
                                      <<list_item_token>>
                                  }
                              }
                          }
                      },
                      None => {
                          return Ok(RefCell::new(Token::EOF));
                      },
                  }
              }
          };

          <<main_loop>>
      }

      <<tests>>
    #+END_SRC

** LL(1) Recursive Descent Lexer - Generic (WIP)
   - above implementation concrete; how would a generic one look ?
   - the lexing algorithm doesn't really change, so it should be reusable

    #+BEGIN_SRC rust
      use std::cell::RefCell;
      struct Lexer {
          iterator: RefCell<Box<dyn Iterator<Item = char>>>,
      }
      impl Lexer {
          fn new(input: &String) -> ListLexer {
              let mut iterator = input.chars().peekable();
              Lexer { iterator: RefCell::new(Box::new(iterator)) }
          }
      }
    #+END_SRC

** LL(1) Recursive Descent Parser
*** Grammar
    #+BEGIN_SRC
      list          : '[' elements ']' ;
      elements  : element (',' element)* ;
      element   : NAME | list ;
    #+END_SRC

*** AST Attempt
      - not sure how to express the above as an AST
      - more precisely, that an element can be either a list item, or another list
        - item element: ~[a]~
        - list element: ~[[a]]~
    #+BEGIN_SRC nim
      type NodeKind = enum
        List,
        Element,
        Elements,

      type ElementNodeKind = enum
        Item,
        List,

      type NodeRef = ref Node
      type Node = object
          case kind: NodeKind # discriminator
          of List: children: Elements,
          of Elements: children seq[Element],
          of Element:
            case kind: ElementNodeKind
            of Item: item: string,
            of List: list: List,
    #+END_SRC

*** Parser that does nothing
    #+BEGIN_SRC nim
      import strutils

      type TokenKind = enum
        Comma,
        SquareBracketOpen,
        SquareBracketClose,
        ListItem,
        EOF,

      type Token = object
        kind: TokenKind
        payload: string

      type Parser = ref object
        input: seq[Token]
        input_cursor: int
        lookahead: Token

      type ParserException = object of Defect

      proc match(this: Parser, token_kind: TokenKind): void =
        if this.lookahead.kind == token_kind:
          echo "Matched a '$#'" % [$token_kind, $this.lookahead]
          if this.input_cursor < len(this.input)-1:
            this.input_cursor = this.input_cursor + 1
            this.lookahead = this.input[this.input_cursor]
        else:
          let msg = "Expected $#, got $#" % [$token_kind, $this.lookahead.kind]
          raise ParserException.newException(msg)

      # forward declarations
      proc elements(this: Parser): void;
      proc element(this: Parser): void;

      proc list(this: Parser): void =
        this.match(SquareBracketOpen)
        this.elements()
        this.match(SquareBracketClose)

      proc elements(this: Parser): void =
        this.element()
        while this.lookahead.kind == Comma:
          this.match(Comma)
          this.element()

      proc element(this: Parser): void =
        if this.lookahead.kind == ListItem:
          this.match(ListItem)
        elif this.lookahead.kind == SquareBracketOpen:
          this.list()

      #proc parse(this: Parser, cb: proc(node: string): void): void =
      proc parse(this: Parser): void =
        this.lookahead = this.input[0]
        this.list()

      # tests
      echo "testing: [ foo ]"
      let parser = Parser(input: @[
        Token(kind: SquareBracketOpen),
        TOken(kind: ListItem, payload: "foo"),
        Token(kind: SquareBracketClose),
      ])
      parser.parse()

      echo "testing: [ foo, bar ]"
      let parser2 = Parser(input: @[
        Token(kind: SquareBracketOpen),
        TOken(kind: ListItem, payload: "foo"),
        Token(kind: Comma),
        TOken(kind: ListItem, payload: "bar"),
        Token(kind: SquareBracketClose),
      ])
      parser2.parse()

      echo "testing: [ [ foo ] ]"
      let parser3 = Parser(input: @[
        Token(kind: SquareBracketOpen),
        Token(kind: SquareBracketOpen),
        TOken(kind: ListItem, payload: "foo"),
        Token(kind: SquareBracketClose),
        Token(kind: SquareBracketClose),
      ])
      parser3.parse()

      echo "testing: [ foo, [ bar, baz ] ]"
      let parser4 = Parser(input: @[
        Token(kind: SquareBracketOpen),
        TOken(kind: ListItem, payload: "foo"),
        Token(kind: Comma),
        Token(kind: SquareBracketOpen),
        TOken(kind: ListItem, payload: "bar"),
        Token(kind: Comma),
        TOken(kind: ListItem, payload: "baz"),
        Token(kind: SquareBracketClose),
        Token(kind: SquareBracketClose),
      ])
      parser4.parse()
  #+END_SRC

  #+RESULTS:
  #+begin_example
  testing: [ foo ]
  Matched a 'SquareBracketOpen'
  Matched a 'ListItem'
  Matched a 'SquareBracketClose'
  testing: [ foo, bar ]
  Matched a 'SquareBracketOpen'
  Matched a 'ListItem'
  Matched a 'Comma'
  Matched a 'ListItem'
  Matched a 'SquareBracketClose'
  testing: [ [ foo ] ]
  Matched a 'SquareBracketOpen'
  Matched a 'SquareBracketOpen'
  Matched a 'ListItem'
  Matched a 'SquareBracketClose'
  Matched a 'SquareBracketClose'
  testing: [ foo, [ bar, baz ] ]
  Matched a 'SquareBracketOpen'
  Matched a 'ListItem'
  Matched a 'Comma'
  Matched a 'SquareBracketOpen'
  Matched a 'ListItem'
  Matched a 'Comma'
  Matched a 'ListItem'
  Matched a 'SquareBracketClose'
  Matched a 'SquareBracketClose'
  #+end_example

*** Tests
  #+BEGIN_SRC nim
  #+END_SRC

** Resources
   - practical terminology intro: https://tomassetti.me/guide-parsing-algorithms-terminology/
   - [[https://www.youtube.com/watch?v=VKM1eLoN-gI][Parsing Algorithms. Lecture [0/0] Abstract Syntax Trees]]
     - CST vs AST
   - [[https://www.youtube.com/watch?v=eF9qWbuQLuw][Parser and Lexer — How to Create a Compiler part 1/5 — Converting text into an Abstract Syntax Tree]]
   - https://www.youtube.com/watch?v=9-EYWLbmiG0
     
